# Introducci√≥n a spaCy

Entramos en **spacy.io/usage** y nos encontramos con los primeros pasos para instalar y usar spaCy correctamente.

---

## 1Ô∏è‚É£ Actualizaci√≥n de herramientas b√°sicas

```bash
pip install -U pip setuptools wheel
```

### ¬øQu√© hace?

Actualiza herramientas b√°sicas del ecosistema Python.

### Herramientas

* **pip**
  Gestor de paquetes. Sirve para instalar librer√≠as:

  ```bash
  pip install X
  ```

* **setuptools**
  Permite que muchos paquetes se *construyan* e instalen correctamente.

* **wheel**
  Define el formato `.whl`, que son paquetes ya compilados ‚Üí instalaci√≥n m√°s r√°pida y menos errores.

---

## 2Ô∏è‚É£ Instalaci√≥n de spaCy

```bash
pip install -U spacy
```

üì¶ Instala o actualiza la librer√≠a **spaCy**.

---

## 3Ô∏è‚É£ Descarga de un modelo de lenguaje

```bash
python -m spacy download en_core_web_sm
```

### `python -m spacy`

Ejecuta spaCy como un **m√≥dulo**, no como un script cualquiera.

Esto garantiza que se usa el spaCy del **entorno activo** (muy importante cuando trabajamos con `venv`).

### `download`

Comando interno de spaCy para descargar modelos.

### `en_core_web_sm`

Modelo de lenguaje:

* **en** ‚Üí ingl√©s
* **core** ‚Üí modelo general
* **web** ‚Üí entrenado con texto web
* **sm** ‚Üí *small* (peque√±o, r√°pido, menos preciso)

### Incluye

* Tokenizador
* POS tagging
* Named Entity Recognition (NER)
* Dependencias sint√°cticas

---

## 4Ô∏è‚É£ Cargando el modelo

```python
nlp = spacy.load("en_core_web_sm")
```

Esto crea el objeto principal de trabajo en spaCy: `nlp`.

---

## 5Ô∏è‚É£ Containers en spaCy

Los **containers** son objetos de spaCy que contienen gran cantidad de informaci√≥n sobre un texto y permiten trabajar con NLP de forma estructurada.

---

### `Language`

Es el **objeto principal de spaCy**.

* Representa el pipeline completo (tokenizer + componentes NLP)
* Normalmente lo llamamos `nlp`

```python
import spacy
nlp = spacy.load("en_core_web_sm")
```

üëâ A partir de `Language` se crean todos los dem√°s objetos.

---

### `Doc`

Representa un **texto procesado** por spaCy.

* Contiene tokens, frases, entidades, dependencias, etc.
* Es el objeto central del an√°lisis NLP

```python
doc = nlp("This is a sentence")
```

üëâ Un `Doc` es **inmutable** (no se pueden a√±adir tokens despu√©s).

---

### `Token`

Representa una **palabra o s√≠mbolo individual** dentro de un `Doc`.

```python
token = doc[0]
print(token.text)
```

Incluye informaci√≥n como:

* Forma original
* Lema
* POS tag
* Dependencias sint√°cticas

---

### `Span`

Representa un **fragmento continuo de texto** dentro de un `Doc`.

```python
span = doc[0:2]
```

Usos t√≠picos:

* Entidades nombradas
* Frases
* Sub-secciones del texto

üëâ Un `Span` **no copia texto**, solo referencia al `Doc` original.

---

### `SpanGroup`

Agrupa m√∫ltiples `Span` relacionados dentro de un `Doc`.

Ejemplo:

* Todas las entidades detectadas
* Spans creados por un componente custom

```python
doc.spans["my_group"] = [span1, span2]
```

---

### `Lexeme`

Representa una **entrada del vocabulario**, no una aparici√≥n concreta.

* Vive en `nlp.vocab`
* Comparte informaci√≥n entre tokens iguales

```python
lexeme = nlp.vocab["apple"]
```

Incluye:

* Forma normalizada
* Flags (is_alpha, is_stop, etc.)

üëâ Un `Lexeme` **no pertenece a un texto espec√≠fico**.

---

### `DocBin`

Contenedor eficiente para **serializar y guardar muchos `Doc`**.

* Muy usado en entrenamiento
* M√°s r√°pido y compacto que guardar texto plano

```python
doc_bin = DocBin()
doc_bin.add(doc)
```

---

### `Example`

Representa un **ejemplo de entrenamiento**.

Contiene:

* Un `Doc` predicho
* Un `Doc` con la anotaci√≥n correcta

Se usa para:

* Entrenar
* Evaluar modelos

```python
from spacy.training import Example
example = Example.from_dict(doc, annotations)
```

---

## üß† Resumen mental r√°pido

* `Language` ‚Üí el motor (pipeline)
* `Doc` ‚Üí texto procesado
* `Token` ‚Üí palabra individual
* `Span` ‚Üí trozo de texto
* `SpanGroup` ‚Üí grupo de spans
* `Lexeme` ‚Üí vocabulario global
* `DocBin` ‚Üí muchos docs empaquetados
* `Example
